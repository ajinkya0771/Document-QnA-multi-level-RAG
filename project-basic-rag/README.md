# ğŸ“„ Project 1 â€” Document Q&A System (Basic RAG with Local Embeddings)

<p>
<img src="https://img.shields.io/badge/Python-3.10+-blue?logo=python" />
<img src="https://img.shields.io/badge/LlamaIndex-RAG-orange?logo=semanticweb" />
<img src="https://img.shields.io/badge/HuggingFace-Embeddings-yellow?logo=huggingface" />
<img src="https://img.shields.io/badge/Gradio-WebUI-green?logo=googlechrome" />
<img src="https://img.shields.io/badge/Docker-Containerized-blue?logo=docker" />
</p>

---

## ğŸ“‘ Table of Contents

- [Introduction](#introduction)
- [Architecture Diagram (Project 1)](#architecture-diagram-project-1)
- [Features](#features)
- [Project Structure](#project-structure)
- [Screenshots](#screenshots)
- [How to Run](#how-to-run)
- [How It Works](#how-it-works)
- [Purpose of This Project](#purpose-of-this-project)

---

## Introduction

This project is a **fully local** Document Question-Answering system using:

- **LlamaIndex**
- **HuggingFace local embedding models**
- **Gradio UI**
- **No external APIs**

It demonstrates a complete offline RAG workflow.

---

## Architecture Diagram (Project 1)

![Basic RAG Architecture](./screenshots/RAG_architecture.png)

---

## Features

### ğŸ”¹ 1. Local Embedding-Based Search  
Uses the **BAAI/bge-small-en-v1.5** embedding model from HuggingFace.

### ğŸ”¹ 2. Document Indexing  
Documents are read via `SimpleDirectoryReader`, embedded, and stored in a `VectorStoreIndex`.

### ğŸ”¹ 3. Interactive Gradio UI  
Allows users to:
- Upload a file  
- Index its content  
- Ask natural-language questions  
- View retrieved answers  

### ğŸ”¹ 4. No LLM Required  
Pure retrieval-based answering, making it:
- Fast  
- Lightweight  
- Free  
- Private  

### ğŸ”¹ 5. Dockerized  
Can be built and run anywhere:

docker build -t doc-qa-basic .
docker run -p 7860:7860 doc-qa-basic

## Project Structure
```
project-basic-rag/
â”‚â”€â”€ app.py
â”‚â”€â”€ Dockerfile
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md
â”‚â”€â”€ docs/
â”‚â”€â”€ screenshots/
â”‚ â”‚â”€â”€ 01-folder-structure.png
â”‚ â”‚â”€â”€ 02-index-document.png
â”‚ â”‚â”€â”€ 03-indexed-status.png
â”‚ â”‚â”€â”€ 04-query-and-answer.png
â”‚ â”‚â”€â”€ RAG_architecture.png
```

## Screenshots

| Step                       | Image |
|---------------------------|--------|
| ğŸ“ **Project Structure**  | ![Folder](screenshots/01-folder-structure.png) |
| ğŸ“¤ **Document Upload & Index** | ![Index](screenshots/02-index-document.png) |
| ğŸ“Œ **Index Confirmation** | ![Indexed](screenshots/03-indexed-status.png) |
| â“ **Query & Final Answer** | ![Answer](screenshots/04-query-and-answer.png) |


## How to Run

### 1ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

2ï¸âƒ£ Start the App
python app.py
Then open:

ğŸ‘‰ http://localhost:7860

3ï¸âƒ£ Run Using Docker
docker build -t doc-qa-basic .
docker run -p 7860:7860 doc-qa-basic


## How It Works
Upload Document â†’ LlamaIndex reads the file

Embed Document â†’ Convert text to vectors using BGE-small

Store in Vector Index â†’ Enables similarity search

User Query â†’ Query is embedded

Similarity Search â†’ Retrieves best matching chunks

Answer Returned â†’ Pure retrieval (no LLM used)

## Purpose of This Project
This project is designed to help to
Understand and implement RAG systems

Learn vector embeddings & similarity search

Build offline, private Q&A pipelines

Prepare for advanced RAG projects (metadata, reranking, hybrid retrieval)

Strengthen your AI/ML, Data Engineering, and GenAI portfolio

Ideal for a fresher showcasing real hands-on RAG experience.
