ğŸ“„ Project 1 â€” Document Q&A System (Basic RAG with Local Embeddings)

This project is a fully local Document Question-Answering system built using LlamaIndex, HuggingFace embedding models, and Gradio.
It demonstrates how to:

ğŸ“¥ Upload and index documents

ğŸ” Convert documents into vector embeddings

â“ Ask questions based on document content

ğŸ’¬ Retrieve accurate answers using similarity search

ğŸš« Run without any external API calls (No OpenAI keys required)

ğŸ³ Package and run the entire app using Docker

This is the first project in my multi-level RAG (Retrieval-Augmented Generation) series, starting from basics and moving toward advanced RAG pipelines.

ğŸš€ Features
ğŸ”¹ 1. Local Embedding-Based Search

Uses the BAAI/bge-small-en-v1.5 embedding model from HuggingFace, running completely offline.

ğŸ”¹ 2. Document Indexing

Uploaded documents are read using SimpleDirectoryReader, converted into vector representations, and stored in a VectorStoreIndex.

ğŸ”¹ 3. Interactive UI with Gradio

User-friendly interface to:

Upload a file

Index the content

Ask natural language questions

View answers

ğŸ”¹ 4. No LLM Required

The system performs pure embedding similarity search, making it:

âœ”ï¸ Fast
âœ”ï¸ Lightweight
âœ”ï¸ Free
âœ”ï¸ Privacy-safe

ğŸ”¹ 5. Dockerized

The entire app can be built and deployed using:

docker build -t doc-qa-basic .
docker run -p 7860:7860 doc-qa-basic

ğŸ—ï¸ Tech Stack
Component	Purpose
Python 3.10+	Core implementation
LlamaIndex	Document loading & vector index
HuggingFace Embeddings	Local embedding model
Gradio	Web UI
Docker	Containerization
ğŸ“ Project Structure
project-basic-rag/
â”‚â”€â”€ app.py
â”‚â”€â”€ Dockerfile
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md
â”‚â”€â”€ docs/
â”‚â”€â”€ screenshots/
â”‚     â”œâ”€â”€ 01-folder-structure.png
â”‚     â”œâ”€â”€ 02-index-document.png
â”‚     â”œâ”€â”€ 03-indexed-status.png
â”‚     â”œâ”€â”€ 04-query-and-answer.png

ğŸ–¼ï¸ Screenshots
Step	Image
ğŸ“ Project Structure	screenshots/01-folder-structure.png
ğŸ“¤ Document Upload & Index	screenshots/02-index-document.png
ğŸ“Œ Index Confirmation	screenshots/03-indexed-status.png
â“ Query & Final Answer	screenshots/04-query-and-answer.png
â–¶ï¸ How to Run
1ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

2ï¸âƒ£ Start the App
python app.py


Then open: http://localhost:7860

3ï¸âƒ£ Using Docker
docker build -t doc-qa-basic .
docker run -p 7860:7860 doc-qa-basic

ğŸ§  How It Works (High-Level Architecture)

Upload Document â†’ Read via LlamaIndex

Embed Document â†’ Convert text into vectors using BGE-small

Store in Vector Index â†’ Efficient similarity search

User Query â†’ Convert question â†’ embedding

Similarity Search â†’ Retrieve relevant chunks

Return Answer â†’ Pure retrieval, no LLM generation

ğŸ¯ Purpose of This Project

This project is designed to:

Build foundational understanding of RAG systems

Learn how vector embeddings work

Build a local, offline, and free Q&A pipeline

Prepare for advanced RAG architectures in Project 2 & 3

Strengthen portfolio for AI/ML Engineer, Data Engineer, and GenAI Engineer roles

It is ideal for a fresher showcasing practical hands-on experience with Retrieval-Augmented Generation.

ğŸ§© Next Steps (Upcoming Projects)

Project 2: Multi-document RAG with metadata filtering

Project 3: Hybrid RAG + LLM (Generation + Retrieval)

Project 4: Enterprise-level Vector Database (Pinecone / ChromaDB)

Project 5: RAG with rerankers, chunking strategies, and evaluation

ğŸ“œ License

This project is open-source and free to use for learning and portfolio purposes.
