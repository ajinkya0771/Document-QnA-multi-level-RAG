ğŸ“„ Project 2 â€” Enterprise Document Q&A System (Advanced RAG)

Tech Stack: LlamaIndex Â· LlamaParse Â· MixedBread Â· Groq Â· Python Â· Vector Stores Â· Gradio
Level: Enterprise / Production-style RAG Pipeline

ğŸ” Overview

This project implements an enterprise-grade Retrieval-Augmented Generation (RAG) system for document question answering.

Unlike basic RAG systems, this pipeline demonstrates:

High-quality document parsing

Structured data normalization

Persistent vector storage

Production-style query engine design

End-to-end execution proof with logs and UI

The system enables users to upload large documents (PDFs), index them into a vector store, and query them using a Groq-powered LLM with MixedBread embeddings.

ğŸ§  Key Capabilities

ğŸ“„ Advanced Document Parsing using LlamaParse

ğŸ§© Structured Chunking & Normalization (JSON-based)

ğŸ”¢ High-quality Embeddings via MixedBread (with HF fallback)

ğŸ—‚ Persistent Vector Store (local disk)

ğŸ” Context-aware Retrieval using LlamaIndex

ğŸ¤– LLM-powered Answer Generation via Groq

ğŸ–¥ Interactive UI for upload, indexing, and querying

ğŸ§¾ Execution Evidence (logs, API usage, terminal proof)

ğŸ—ï¸ Architecture (High Level)
PDF / TXT Document
        â†“
LlamaParse (High-Quality Parsing)
        â†“
Structured Output (Text + Tables + Metadata)
        â†“
Normalized JSON Chunks
        â†“
MixedBread Embeddings
        â†“
Persistent Vector Store
        â†“
Query Engine (LlamaIndex)
        â†“
Groq LLM Response
        â†“
UI / Terminal Output

ğŸ“ Project Structure
project_enterprise_rag/
â”‚â”€â”€ embeddings/        # Embedding logic & configs
â”‚â”€â”€ ingestion/         # Parsing, validation, normalization
â”‚â”€â”€ llm_chain/         # Groq LLM client & prompt logic
â”‚â”€â”€ rag/               # Query engine & retrieval logic
â”‚â”€â”€ storage/           # Persistent vector store files
â”‚â”€â”€ frontend/          # UI components
â”‚â”€â”€ screenshots/       # End-to-end execution proof
â”‚â”€â”€ app.py             # UI entry point
â”‚â”€â”€ api.py             # API entry point (future FastAPI)
â”‚â”€â”€ Dockerfile         # Containerization support
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md

ğŸ§ª Execution Flow â€” Step-by-Step (Screenshots)

The following screenshots demonstrate the complete enterprise RAG pipeline, executed end to end.

ğŸ”¹ Setup & Configuration

Folder Structure (VS Code)

MixedBread API Key Configuration

Groq API Key Configuration

LlamaCloud API Key Setup

ğŸ”¹ Document Parsing (LlamaParse)

Raw Document Before Parsing

LlamaParse Playground

LlamaParse Code Snippet (Python â€“ Basic)

LlamaParse Code Snippet (Python â€“ Full)

Parsed Structured Output

Normalized JSON Chunks

Table Layout Validation

ğŸ”¹ Vector Store Creation

Vector Store Created (Empty)

Add Files Dialog

Parsing Strategy Selection

Store After Upload (Processing â†’ Completed)

ğŸ”¹ Embeddings & Indexing

Embedding Playground Output

Vector Index Created & Persisted (Local)

Vector Store After Indexing

ğŸ”¹ Query & Answering

Query Engine Code Implementation

RAG Query Test (Terminal)

Uploading Document via UI

Chat Response (Answer Generation)

Groq Logs (Response Evidence)

ğŸ“‚ All screenshots are available in the /screenshots folder in sequential order.

â–¶ï¸ How to Run Locally
1ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

2ï¸âƒ£ Set Environment Variables

Create a .env file:

GROQ_API_KEY=your_key_here
MIXEDBREAD_API_KEY=your_key_here
LLAMACLOUD_API_KEY=your_key_here

3ï¸âƒ£ Run the Application
python app.py


Open:

http://localhost:7860

ğŸ¯ Purpose of This Project

This project is designed to showcase real enterprise RAG skills, including:

Production-style data ingestion

Parsing unstructured documents into structured formats

Embedding strategy selection

Vector persistence and retrieval

LLM orchestration

Debugging with logs and execution evidence

It is intentionally built to go beyond tutorials and reflect how RAG systems are implemented in real-world AI teams.

ğŸš€ Whatâ€™s Next (Planned Enhancements)

âœ… FastAPI-based backend API

âœ… Fully Dockerized deployment

â­ Multi-document ingestion

â­ Authentication & rate limiting

â­ Cloud deployment (optional)

ğŸ§‘â€ğŸ’» Author

Ajinkya Dhote
AI / ML Engineer (Fresher)
Focused on GenAI, RAG Systems, and Production ML
